{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "# import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "except:\n",
    "    print('matplotlib importation passed')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as core_layers\n",
    "try:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "except: \n",
    "    print('Axes3D importation passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reset_graph in module __main__:\n",
      "\n",
      "reset_graph(seed_=42)\n",
      "    Utility function to reset current tensorflow computation graph and set the random seed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def reset_graph(seed_=42):\n",
    "    \"\"\"\n",
    "    Utility function to reset current tensorflow computation graph and set the random seed.\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed_)\n",
    "    np.random.seed(seed_)\n",
    "\n",
    "help(reset_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## We use artificial data for the following two specifications of regression:\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "$ y(x) = a + b_1 \\cdot X_1 + b_2 \\cdot X_2 + b_3 \\cdot X_3 + \\sigma \\cdot \\varepsilon $ \n",
    "\n",
    "where $ \\varepsilon \\sim N(0, 1) $ is a Gaussian noise, and $ \\sigma $ is its volatility, \n",
    "with the following choice of parameters:\n",
    "\n",
    "$ a = 1.0 $\n",
    "\n",
    "$ b_1, b_2, b_3 = (0.5, 0.2, 0.1) $\n",
    "\n",
    "$ \\sigma = 0.1 $\n",
    "\n",
    "$ X_1, X_2, X_3 $ will be uniformally distributed in $ [-1,1] $\n",
    "\n",
    "### Non-Linear Regression\n",
    "\n",
    "$ y(x) = a + w_{00} \\cdot X_1 + w_{01} \\cdot X_2 + w_{02} \\cdot X_3 + w_{10} \\cdot X_1^2 + w_{11} \\cdot X_2^2 + w_{12} \\cdot X_3^2 +  \\sigma \\cdot \\varepsilon $ \n",
    "\n",
    "where\n",
    "\n",
    "$ w = [[1.0, 0.5, 0.2],[0.5, 0.3, 0.15]]  $\n",
    "\n",
    "and the rest of parameters is as above, with the same values of $ X_i $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7500, 3), (7500, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_data(n_points_=10000, n_features_=3, use_nonlinear_=True, noise_std_=0.1, train_test_split_=4):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        n_points_ - the number of data points to generate\n",
    "        n_features_ - a positive int \n",
    "        use_nonlinear_ - generate non-linear data if true\n",
    "        train_test_split_ - a portion of data to test\n",
    "\n",
    "    Return:\n",
    "        X_train, Y_train, X_test, Y_test, n_train, n_features\n",
    "    \"\"\"\n",
    "    if use_nonlinear_:\n",
    "        weights = np.array([[1.0, 0.5, 0.2], [0.5, 0.3, 0.15]])\n",
    "    else:\n",
    "        weights = np.array([1.0, 0.5, 0.2])\n",
    "\n",
    "    bias = np.ones(n_points_).reshape(-1, 1)  # 'a' - a vector of size (n_points_, ) \n",
    "    low = - np.ones( (n_points_, n_features_), 'float' )  # a vector of size (n_points_, ) \n",
    "    high = np.ones( (n_points_, n_features_), 'float' )  # an array of size (n_points_, n_features_) \n",
    "\n",
    "    np.random.seed(42)\n",
    "    X = np.random.uniform( low=low, high=high )  # X_1, X_2, X_3 will be uniformally distributed in [-1,1]\n",
    "\n",
    "    np.random.seed(42)\n",
    "    noise = np.random.normal(size=(n_points_, 1))  # e is Gaussian noise\n",
    "    # noise_std_ = 0.1  # ?? why again\n",
    "\n",
    "    if use_nonlinear_:\n",
    "        Y = bias + \\\n",
    "            np.dot(X, weights[0, :]).reshape(-1, 1) + \\\n",
    "            np.dot(X*X, weights[1, :]).reshape(-1, 1) + \\\n",
    "            noise_std_*noise\n",
    "    else:\n",
    "        Y = bias + \\\n",
    "            np.dot(X, weights).reshape(-1, 1) + \\\n",
    "            noise_std_*noise\n",
    "    \n",
    "    n_test = int(n_points_/train_test_split_)\n",
    "    n_train = n_points_ - n_test\n",
    "    \n",
    "    X_train = X[:n_train,:]\n",
    "    Y_train = Y[:n_train].reshape((-1,1))\n",
    "\n",
    "    X_test = X[n_train:,:]\n",
    "    Y_test = Y[n_train:].reshape((-1,1))\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, n_train, n_features_\n",
    "\n",
    "X_train, Y_train, X_test, Y_test, n_train, n_features = generate_data(use_nonlinear_=False)\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Linear Regression with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_lin_regress(X_train_, Y_train_):\n",
    "    \"\"\"\n",
    "    Implements linear regression model using numpy module\n",
    "\n",
    "    Parameters:\n",
    "        X_train_ - np.array of size (n by k) where n is number of observations of independent variables and k is number of variables\n",
    "        Y_train_ - np.array of size (n by 1) where n is the number of observations of dependend variable\n",
    "    \n",
    "    Return:\n",
    "        np.array of size (k+1 by 1) of regression coefficients - 변수에 대한 선형회귀의 계수\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    number_of_features = len(X_train_[0])  # 7500 elements of size 3, n_train\n",
    "    X = np.hstack( (np.ones( len(X_train_) ).reshape(-1, 1), X_train_ ) )  # # add the column of ones\n",
    "    theta_numpy = np.linalg.inv( X.T.dot(X) ).dot(X.T).dot(Y_train_)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return theta_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99946227],\n",
       "       [0.99579039],\n",
       "       [0.499198  ],\n",
       "       [0.20019798]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_lin_regress(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linear Regression with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_lin_regress(X_train_, Y_train_):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        X_train  - np.array of size (n by k) where n is number of observations of independent variables and k is number of variables\n",
    "        Y_train - np.array of size (n by 1) where n is the number of observations of dependend variable\n",
    "    \n",
    "    Return:\n",
    "        np.array of size (k+1 by 1) of regression coefficients\n",
    "    \"\"\" \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lin_reg = LinearRegression()\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # use lin_reg to fit training data\n",
    "    lin_reg.fit(X_train_, Y_train_)\n",
    "    theta_sklearn = np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T]\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return theta_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99946227],\n",
       "       [0.99579039],\n",
       "       [0.499198  ],\n",
       "       [0.20019798]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_lin_regress(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lin_regress(X_train_, Y_train_):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        X_train - np.array of size (n by k) where n is number of observations of independent variables and k is number of variables\n",
    "        Y_train - np.array of size (n by 1) where n is the number of observations of dependend variable\n",
    "    \n",
    "    Return:\n",
    "        np.array of size (k+1 by 1) of regression coefficients\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 7-8 lines of code)\n",
    "    X_np = np.hstack((np.ones(len(X_train_)).reshape((-1, 1)), X_train_))  # add the column of ones\n",
    "    \n",
    "    # define theta for later evaluation\n",
    "    X = tf.constant(X_np, dtype=tf.float32, name='X')\n",
    "    Y = tf.constant(Y_train_, dtype=tf.float32, name='Y')\n",
    "    XT = tf.transpose(X)\n",
    "\n",
    "    theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), Y)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    with tf.Session() as sess:\n",
    "        theta_value = theta.eval()\n",
    "\n",
    "    return theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9994622 ],\n",
       "       [0.9957907 ],\n",
       "       [0.49919802],\n",
       "       [0.20019804]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_lin_regress(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinRegressNormalEq:\n",
    "    \"\"\"\n",
    "    Implements the normal equation, MLE\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features_, lr_=0.05, L_=0):\n",
    "        import math as m\n",
    "        self.X = tf.placeholder(tf.float32, [None, n_features_], name='X')\n",
    "        self.Y = tf.placeholder(tf.float32, [None, 1], name='Y')\n",
    "        self.theta_in = tf.placeholder(tf.float32, [n_features_+1, None])\n",
    "        \n",
    "        data_plus_bias = tf.concat([tf.ones([tf.shape(self.X)[0], 1]), self.X], axis=1)\n",
    "\n",
    "        XT = tf.transpose(data_plus_bias)\n",
    "\n",
    "        self.theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, data_plus_bias)), XT), self.Y)\n",
    "        self.lr_mse = tf.reduce_mean(tf.square(tf.matmul(data_plus_bias, self.theta_in) - self.Y))\n",
    "\n",
    "        self.weights = tf.Variable(tf.random_normal([n_features+2, 1]))\n",
    "        self.output = tf.matmul(data_plus_bias, self.weights[:-1, :])\n",
    "\n",
    "        gauss = tf.distributions.Normal(loc=0.0, scale=1.0)\n",
    "\n",
    "        sigma = 0.0001 + tf.square(self.weights[-1])  # last model weight\n",
    "        \n",
    "        pi = tf.constant(m.pi)\n",
    "\n",
    "        log_LL = tf.log(0.00001 + ( 1/( tf.sqrt(2*pi)*sigma)) * gauss.prob((self.Y - self.output) / sigma ) )  \n",
    "        self.loss = - tf.reduce_mean(log_LL)\n",
    "\n",
    "        self.train_step = (tf.train.AdamOptimizer(lr_).minimize(self.loss), -self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_normal_eq(X_train_, Y_train_, X_test_, Y_test_, lr_=0.05):\n",
    "    \"\"\"\n",
    "    Implements normal equation using tf, trains the model using training set. Tests the model quality by computing MSE of the test data set.\n",
    "\n",
    "    Parameter: \n",
    "        X_train_ - np.array of size (n, k)\n",
    "        Y_train_ - np.array of size (n, 1)\n",
    "        X_test_ - np.array of size (n, k)\n",
    "        Y_test_ - np.array of size (n, 1)\n",
    "\n",
    "    Return:\n",
    "        - np.array of size (k+1, 1) of regression coefficients\n",
    "        - MSE of the test data set\n",
    "        - MSE of the train data set\n",
    "    \"\"\"\n",
    "    n_features = X_train_.shape[1]\n",
    "    model = LinRegressNormalEq(n_features_=n_features, lr_=lr_)\n",
    "\n",
    "    # train the model\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # Normal equation for Linear Regression\n",
    "        (_ , loss), weights = sess.run((model.train_step, model.weights), feed_dict={\n",
    "                model.X: X_train_,\n",
    "                model.Y: Y_train_\n",
    "                })\n",
    "\n",
    "        theta_value = model.theta\n",
    "        lr_mse_train = model.lr_mse\n",
    "        lr_mse_test = model.lr_mse\n",
    "\n",
    "    return theta_value, lr_mse_train, lr_mse_test\n",
    "\n",
    "theta_value, lr_mse_train, lr_mse_test = run_normal_eq(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__enter__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-611ccfa7f78d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_normal_eq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-62630a3f5bf9>\u001b[0m in \u001b[0;36mrun_normal_eq\u001b[1;34m(X_train_, Y_train_, X_test_, Y_test_, lr_)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_train_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Normal equation for Linear Regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: __enter__"
     ]
    }
   ],
   "source": [
    "run_normal_eq(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
